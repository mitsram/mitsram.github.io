<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unified Prompt Evaluation System - Documentation</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --primary-color: #2563eb;
            --primary-dark: #1d4ed8;
            --secondary-color: #7c3aed;
            --success-color: #059669;
            --warning-color: #d97706;
            --danger-color: #dc2626;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-color: #1e293b;
            --text-muted: #64748b;
            --border-color: #e2e8f0;
            --code-bg: #f1f5f9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            padding: 60px 0;
            text-align: center;
        }

        header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
        }

        header .subtitle {
            font-size: 1.1rem;
            opacity: 0.9;
            max-width: 700px;
            margin: 0 auto;
        }

        /* Navigation */
        nav {
            background: var(--card-bg);
            border-bottom: 1px solid var(--border-color);
            padding: 15px 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 5px 20px;
        }

        nav a {
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.9rem;
            padding: 5px 10px;
            border-radius: 5px;
            transition: all 0.2s;
        }

        nav a:hover {
            background: var(--primary-color);
            color: white;
        }

        /* Main Content */
        main {
            padding: 40px 0;
        }

        section {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--primary-color);
            font-size: 1.8rem;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--primary-color);
        }

        h3 {
            color: var(--text-color);
            font-size: 1.4rem;
            margin: 25px 0 15px;
        }

        h4 {
            color: var(--text-muted);
            font-size: 1.1rem;
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
        }

        ul, ol {
            margin: 15px 0;
            padding-left: 25px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 12px 15px;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: var(--code-bg);
        }

        tr:hover {
            background: #e0e7ff;
        }

        /* Code blocks */
        pre {
            background: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            overflow-x: auto;
            margin: 15px 0;
        }

        code {
            font-family: 'Fira Code', 'Monaco', 'Consolas', monospace;
            font-size: 0.9rem;
        }

        :not(pre) > code {
            background: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            color: var(--danger-color);
        }

        /* Feature cards */
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .feature-card {
            background: linear-gradient(135deg, #f0f9ff, #e0f2fe);
            border: 1px solid #bae6fd;
            border-radius: 10px;
            padding: 20px;
        }

        .feature-card h4 {
            color: var(--primary-color);
            margin-top: 0;
        }

        .feature-card p {
            color: var(--text-muted);
            margin-bottom: 0;
            font-size: 0.95rem;
        }

        /* Mermaid diagrams */
        .mermaid {
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            text-align: center;
        }

        /* Info boxes */
        .info-box {
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .info-box.note {
            background: #eff6ff;
            border-left: 4px solid var(--primary-color);
        }

        .info-box.warning {
            background: #fffbeb;
            border-left: 4px solid var(--warning-color);
        }

        .info-box.success {
            background: #f0fdf4;
            border-left: 4px solid var(--success-color);
        }

        .info-box h4 {
            margin-top: 0;
            color: inherit;
        }

        /* Badge */
        .badge {
            display: inline-block;
            padding: 3px 10px;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 600;
        }

        .badge.required {
            background: var(--danger-color);
            color: white;
        }

        .badge.optional {
            background: var(--text-muted);
            color: white;
        }

        /* Footer */
        footer {
            background: var(--text-color);
            color: white;
            padding: 30px 0;
            text-align: center;
        }

        footer p {
            opacity: 0.8;
            margin-bottom: 5px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8rem;
            }

            nav ul {
                flex-direction: column;
                align-items: center;
            }

            section {
                padding: 20px;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: 8px 10px;
            }
        }

        /* Print styles */
        @media print {
            nav {
                display: none;
            }

            section {
                break-inside: avoid;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>üîç Unified Prompt Evaluation System</h1>
            <p class="subtitle">A comprehensive, extensible system for evaluating LLM outputs across multiple categories with separated prompt engineering and input handling.</p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#architecture">Architecture</a></li>
                <li><a href="#data-model">Data Model</a></li>
                <li><a href="#excel-structure">Excel Structure</a></li>
                <li><a href="#pipeline">Pipeline</a></li>
                <li><a href="#metrics">Metrics</a></li>
                <li><a href="#llm-providers">LLM Providers</a></li>
                <li><a href="#evaluators">Evaluators</a></li>
                <li><a href="#usage">Usage</a></li>
            </ul>
        </div>
    </nav>

    <main class="container">
        <!-- Overview -->
        <section id="overview">
            <h2>üìã System Overview</h2>
            
            <h3>Objective</h3>
            <p>A <strong>unified, extensible prompt evaluation system</strong> that evaluates LLM outputs across multiple categories:</p>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>üìù Requirement Analysis</h4>
                    <p>Evaluating generated IT requirements from user stories and specifications</p>
                </div>
                <div class="feature-card">
                    <h4>üß™ Test Case Design</h4>
                    <p>Evaluating generated test cases and scenarios for coverage and quality</p>
                </div>
                <div class="feature-card">
                    <h4>üíª Automated Test Scripts</h4>
                    <p>Evaluating generated test automation code for syntax and compliance</p>
                </div>
                <div class="feature-card">
                    <h4>üîß Custom Categories</h4>
                    <p>Support for any user-defined evaluation category</p>
                </div>
            </div>

            <h3>Key Features</h3>
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Single Codebase</strong></td>
                        <td>One system handles all evaluation types</td>
                    </tr>
                    <tr>
                        <td><strong>Excel-Based Input</strong></td>
                        <td>Easy-to-use Excel files with category column</td>
                    </tr>
                    <tr>
                        <td><strong>Separated Prompt/Input</strong></td>
                        <td><code>prompt_template</code> + <code>prompt_input</code> for clear evaluation</td>
                    </tr>
                    <tr>
                        <td><strong>Placeholder Support</strong></td>
                        <td><code>{{input}}</code> substitution in templates</td>
                    </tr>
                    <tr>
                        <td><strong>Pluggable Metrics</strong></td>
                        <td>Register new metrics without modifying core code</td>
                    </tr>
                    <tr>
                        <td><strong>Multi-LLM Support</strong></td>
                        <td>OpenAI, Azure, Anthropic, Google, local models</td>
                    </tr>
                    <tr>
                        <td><strong>HTML Reports</strong></td>
                        <td>Beautiful, interactive HTML reports with charts</td>
                    </tr>
                    <tr>
                        <td><strong>CI/CD Ready</strong></td>
                        <td>Command-line interface for automation pipelines</td>
                    </tr>
                </tbody>
            </table>

            <h3>Prompt Evaluation Flow</h3>
            <p>The system separates the prompt engineering template from the actual input data for clear evaluation:</p>
            
            <div class="mermaid">
flowchart TB
    subgraph Input["üì• Input Fields"]
        PT[/"prompt_template<br/>You are a requirements analyst...<br/>User Story: {{input}}"/]
        PI[/"prompt_input<br/>As a customer, I want to login..."/]
    end

    subgraph Processing["‚öôÔ∏è Processing"]
        SUB["Placeholder Substitution<br/>{{input}} ‚Üí actual input"]
    end

    subgraph Output["üì§ Output"]
        RP[/"rendered_prompt<br/>(computed property)"/]
        LLM["LLM Processing"]
        OUT[/"llm_output<br/>REQ-001: The system shall..."/]
    end

    subgraph Context["üìö Context"]
        RC[/"retrieval_context<br/>Source documents for<br/>faithfulness evaluation"/]
    end

    PT --> SUB
    PI --> SUB
    SUB --> RP
    RP --> LLM
    LLM --> OUT
    RC -.->|"Used by Faithfulness Metric"| OUT

    style PT fill:#e0f2fe,stroke:#0284c7
    style PI fill:#dcfce7,stroke:#16a34a
    style RP fill:#fef3c7,stroke:#d97706
    style OUT fill:#f3e8ff,stroke:#9333ea
    style RC fill:#fee2e2,stroke:#dc2626
            </div>

            <h3>Benefits of Separated Prompt/Input</h3>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>üéØ Prompt Engineering Evaluation</h4>
                    <p>Evaluate the template quality independently from input data</p>
                </div>
                <div class="feature-card">
                    <h4>üîÑ Input Variation Testing</h4>
                    <p>Test the same template with different inputs easily</p>
                </div>
                <div class="feature-card">
                    <h4>üìä Clear Traceability</h4>
                    <p>Track what template + input produced what output</p>
                </div>
                <div class="feature-card">
                    <h4>üîó RAG Support</h4>
                    <p>Explicit <code>retrieval_context</code> for faithfulness evaluation</p>
                </div>
            </div>

            <h3>Placeholder Syntax</h3>
            <table>
                <thead>
                    <tr>
                        <th>Placeholder</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>{{input}}</code></td>
                        <td>Main input placeholder</td>
                        <td><code>Analyze: {{input}}</code></td>
                    </tr>
                    <tr>
                        <td><code>{{ input }}</code></td>
                        <td>With spaces (also supported)</td>
                        <td><code>Analyze: {{ input }}</code></td>
                    </tr>
                    <tr>
                        <td><code>{{custom_name}}</code></td>
                        <td>Named placeholders (from metadata)</td>
                        <td><code>Role: {{role}}</code></td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Architecture -->
        <section id="architecture">
            <h2>üèóÔ∏è System Architecture</h2>

            <h3>High-Level Architecture</h3>
            <div class="mermaid">
flowchart TB
    subgraph Input["üì• Data Input"]
        EXCEL[("Excel Dataset")]
    end

    subgraph Loader["üìÇ Data Loader"]
        PARSE["Excel Parser"]
        MAP["Column Mapper"]
        VAL["Validation Engine"]
        BUILD["EvaluationItem Builder"]
    end

    subgraph Items["üì¶ Evaluation Items"]
        ITEM["EvaluationItem<br/>‚Ä¢ prompt_template<br/>‚Ä¢ prompt_input<br/>‚Ä¢ rendered_prompt<br/>‚Ä¢ llm_output<br/>‚Ä¢ retrieval_context"]
    end

    subgraph Router["üîÄ Evaluation Router"]
        REQ["Requirement<br/>Evaluator"]
        TC["Test Case<br/>Evaluator"]
        TS["Test Script<br/>Evaluator"]
        CUSTOM["Custom<br/>Evaluator"]
    end

    subgraph Metrics["üìä Metrics"]
        DEEP["DeepEval Metrics<br/>‚Ä¢ Faithfulness<br/>‚Ä¢ Relevancy<br/>‚Ä¢ G-Eval"]
        NLTK["NLTK Metrics<br/>‚Ä¢ Readability<br/>‚Ä¢ Terminology<br/>‚Ä¢ Ambiguity"]
        CUST["Custom Metrics<br/>‚Ä¢ Format<br/>‚Ä¢ Syntax<br/>‚Ä¢ Coverage"]
    end

    subgraph Aggregator["üìà Score Aggregator"]
        WEIGHT["Weighted Combination"]
        THRESH["Threshold Checker"]
        STATS["Statistics Generator"]
    end

    subgraph Report["üìÑ Report Generator"]
        HTML["HTML Report"]
        JSON["JSON Export"]
    end

    EXCEL --> PARSE
    PARSE --> MAP
    MAP --> VAL
    VAL --> BUILD
    BUILD --> ITEM

    ITEM --> REQ
    ITEM --> TC
    ITEM --> TS
    ITEM --> CUSTOM

    REQ --> DEEP
    REQ --> NLTK
    TC --> DEEP
    TC --> NLTK
    TS --> DEEP
    TS --> CUST
    CUSTOM --> DEEP

    DEEP --> WEIGHT
    NLTK --> WEIGHT
    CUST --> WEIGHT

    WEIGHT --> THRESH
    THRESH --> STATS
    STATS --> HTML
    STATS --> JSON

    style ITEM fill:#fef3c7,stroke:#d97706
    style DEEP fill:#dbeafe,stroke:#2563eb
    style NLTK fill:#dcfce7,stroke:#16a34a
    style CUST fill:#f3e8ff,stroke:#9333ea
            </div>

            <h3>Core Design Principles</h3>
            <table>
                <thead>
                    <tr>
                        <th>Principle</th>
                        <th>Implementation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Single Responsibility</strong></td>
                        <td>Each module handles one specific task</td>
                    </tr>
                    <tr>
                        <td><strong>Open/Closed</strong></td>
                        <td>Extend via plugins, don't modify core</td>
                    </tr>
                    <tr>
                        <td><strong>Dependency Injection</strong></td>
                        <td>Metrics and evaluators are injected</td>
                    </tr>
                    <tr>
                        <td><strong>Strategy Pattern</strong></td>
                        <td>Different evaluation strategies per category</td>
                    </tr>
                    <tr>
                        <td><strong>Factory Pattern</strong></td>
                        <td>Create evaluators based on category</td>
                    </tr>
                    <tr>
                        <td><strong>Registry Pattern</strong></td>
                        <td>Central registry for metrics and validators</td>
                    </tr>
                </tbody>
            </table>

            <h3>Component Interactions</h3>
            <div class="mermaid">
flowchart LR
    subgraph Core["Core Components"]
        ENGINE["EvaluationEngine"]
        LOADER["DataLoader"]
        REGISTRY["MetricRegistry"]
        REPORT["ReportGenerator"]
    end

    subgraph Metrics["Metric Types"]
        DEEPEVAL["DeepEval Metrics"]
        NLTKM["NLTK Metrics"]
    end

    ENGINE --> LOADER
    ENGINE --> REGISTRY
    ENGINE --> REPORT

    REGISTRY --> DEEPEVAL
    REGISTRY --> NLTKM

    LOADER -->|"load & validate"| ENGINE

    style ENGINE fill:#dbeafe,stroke:#2563eb
    style REGISTRY fill:#fef3c7,stroke:#d97706
            </div>
        </section>

        <!-- Data Model -->
        <section id="data-model">
            <h2>üì¶ Data Model & Schema</h2>

            <h3>Core Data Model: EvaluationItem</h3>
            <p>The <code>EvaluationItem</code> dataclass is the central data structure:</p>

            <div class="mermaid">
classDiagram
    class EvaluationItem {
        +str id
        +str category
        +str prompt_template
        +str prompt_input
        +str llm_output
        +str expected_output
        +str retrieval_context
        +str framework
        +str language
        +str llm_provider
        +str llm_model
        +str requirement_id
        +str subcategory
        +str priority
        +List~str~ tags
        +str notes
        +Dict metadata
        +rendered_prompt() str
        +to_dict() Dict
    }

    class MetricResult {
        +str metric_name
        +float score
        +bool passed
        +float threshold
        +str reason
        +Dict details
        +MetricType metric_type
        +float execution_time_ms
    }

    class EvaluationResult {
        +str item_id
        +str category
        +float overall_score
        +bool passed
        +List~MetricResult~ metric_results
        +str llm_provider_used
        +str llm_model_used
        +List~str~ errors
        +List~str~ warnings
        +List~str~ recommendations
    }

    class EvaluationSummary {
        +int total_items
        +int passed_items
        +int failed_items
        +float pass_rate
        +float average_score
        +Dict category_summaries
        +Dict metric_summaries
    }

    EvaluationItem "1" --> "*" EvaluationResult : evaluated by
    EvaluationResult "1" --> "*" MetricResult : contains
    EvaluationResult "*" --> "1" EvaluationSummary : aggregates to
            </div>

            <h3>EvaluationItem Implementation</h3>
            <pre><code>@dataclass
class EvaluationItem:
    """
    Single item to evaluate.
    
    The evaluation system separates prompt engineering from input data:
    - prompt_template: The engineered prompt with {{input}} placeholders
    - prompt_input: The actual input data to be inserted into the template
    - llm_output: The LLM's response to the rendered prompt
    - expected_output: Optional ground truth for comparison metrics
    - retrieval_context: Reference documents for faithfulness evaluation
    """
    # Required fields
    id: str                          # Unique identifier (e.g., "EVAL-001")
    category: str                    # Category type (e.g., "requirement")
    prompt_template: str             # Prompt engineering template with {{input}}
    prompt_input: str                # Actual input data to substitute
    llm_output: str                  # LLM's generated response
    
    # Optional fields
    expected_output: Optional[str]   # Ground truth for comparison
    retrieval_context: Optional[str] # Source documents for faithfulness
    framework: Optional[str]         # For test_script: playwright, selenium, etc.
    language: Optional[str]          # Programming language
    llm_provider: Optional[str]      # Override LLM provider
    llm_model: Optional[str]         # Override LLM model
    # ... additional fields
    
    @property
    def rendered_prompt(self) -> str:
        """Get the fully rendered prompt with input substituted."""
        prompt = self.prompt_template
        prompt = prompt.replace("{{input}}", self.prompt_input)
        prompt = prompt.replace("{{ input }}", self.prompt_input)
        # Handle named placeholders from metadata...
        return prompt</code></pre>

            <h3>Field Usage by Component</h3>
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Fields Used</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>DeepEval Metrics</strong></td>
                        <td><code>rendered_prompt</code>, <code>llm_output</code>, <code>retrieval_context</code></td>
                        <td>LLM-based evaluation</td>
                    </tr>
                    <tr>
                        <td><strong>NLTK Metrics</strong></td>
                        <td><code>rendered_prompt</code>, <code>llm_output</code></td>
                        <td>Text analysis</td>
                    </tr>
                    <tr>
                        <td><strong>Custom Metrics</strong></td>
                        <td>All fields as needed</td>
                        <td>Custom validation</td>
                    </tr>
                    <tr>
                        <td><strong>Validators</strong></td>
                        <td><code>prompt_template</code>, <code>prompt_input</code></td>
                        <td>Input validation</td>
                    </tr>
                    <tr>
                        <td><strong>Reports</strong></td>
                        <td>All fields</td>
                        <td>Display and analysis</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Excel Structure -->
        <section id="excel-structure">
            <h2>üìä Excel Dataset Structure</h2>

            <h3>Required Columns</h3>
            <table>
                <thead>
                    <tr>
                        <th>Column</th>
                        <th>Type</th>
                        <th>Required</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>id</code></td>
                        <td>String</td>
                        <td><span class="badge required">Yes</span></td>
                        <td>Unique identifier (e.g., EVAL-001)</td>
                    </tr>
                    <tr>
                        <td><code>category</code></td>
                        <td>String</td>
                        <td><span class="badge required">Yes</span></td>
                        <td><code>requirement</code>, <code>test_case</code>, <code>test_script</code>, or custom</td>
                    </tr>
                    <tr>
                        <td><code>prompt_template</code></td>
                        <td>Text</td>
                        <td><span class="badge required">Yes</span></td>
                        <td>Prompt engineering template with <code>{{input}}</code> placeholders</td>
                    </tr>
                    <tr>
                        <td><code>prompt_input</code></td>
                        <td>Text</td>
                        <td><span class="badge required">Yes</span></td>
                        <td>Input data to substitute into the template</td>
                    </tr>
                    <tr>
                        <td><code>llm_output</code></td>
                        <td>Text</td>
                        <td><span class="badge required">Yes</span></td>
                        <td>The LLM's generated response to evaluate</td>
                    </tr>
                    <tr>
                        <td><code>retrieval_context</code></td>
                        <td>Text</td>
                        <td><span class="badge optional">No</span></td>
                        <td>Source documents for faithfulness evaluation</td>
                    </tr>
                    <tr>
                        <td><code>expected_output</code></td>
                        <td>Text</td>
                        <td><span class="badge optional">No</span></td>
                        <td>Ground truth for comparison metrics</td>
                    </tr>
                    <tr>
                        <td><code>framework</code></td>
                        <td>String</td>
                        <td><span class="badge optional">Conditional</span></td>
                        <td>For test_script: <code>playwright</code>, <code>selenium</code>, etc.</td>
                    </tr>
                    <tr>
                        <td><code>llm_provider</code></td>
                        <td>String</td>
                        <td><span class="badge optional">No</span></td>
                        <td>Override default: <code>openai</code>, <code>azure</code>, <code>anthropic</code></td>
                    </tr>
                    <tr>
                        <td><code>llm_model</code></td>
                        <td>String</td>
                        <td><span class="badge optional">No</span></td>
                        <td>Override default model</td>
                    </tr>
                </tbody>
            </table>

            <h3>Example Data Structure</h3>
            <div class="mermaid">
graph LR
    subgraph Row["Excel Row Example"]
        ID["id: EVAL-001"]
        CAT["category: requirement_analysis"]
        PT["prompt_template:<br/>Analyze user story: {{input}}"]
        PI["prompt_input:<br/>As a user, I want to login..."]
        OUT["llm_output:<br/>REQ-001: System shall..."]
        RC["retrieval_context:<br/>Auth spec v2.1..."]
    end

    style PT fill:#e0f2fe,stroke:#0284c7
    style PI fill:#dcfce7,stroke:#16a34a
    style OUT fill:#f3e8ff,stroke:#9333ea
    style RC fill:#fee2e2,stroke:#dc2626
            </div>

            <h3>Glossary Sheet (Optional)</h3>
            <p>The system supports a separate "Glossary" sheet for domain terminology:</p>
            <table>
                <thead>
                    <tr>
                        <th>Column</th>
                        <th>Type</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>term</code></td>
                        <td>String</td>
                        <td>Domain term</td>
                    </tr>
                    <tr>
                        <td><code>definition</code></td>
                        <td>String</td>
                        <td>Term definition</td>
                    </tr>
                    <tr>
                        <td><code>synonyms</code></td>
                        <td>String</td>
                        <td>Comma-separated synonyms</td>
                    </tr>
                    <tr>
                        <td><code>category</code></td>
                        <td>String</td>
                        <td>Which category uses this term</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Pipeline -->
        <section id="pipeline">
            <h2>‚öôÔ∏è Evaluation Pipeline</h2>

            <h3>Pipeline Flow</h3>
            <div class="mermaid">
flowchart TB
    subgraph Step1["Step 1: Initialization"]
        INIT1["Load configuration (YAML/ENV)"]
        INIT2["Initialize LLM Provider Manager"]
        INIT3["Download NLTK resources"]
        INIT4["Register metrics in MetricRegistry"]
    end

    subgraph Step2["Step 2: Data Loading"]
        LOAD1["Read Excel file"]
        LOAD2["Validate required columns"]
        LOAD3["Parse into EvaluationItem objects"]
        LOAD4["Compute rendered_prompt"]
    end

    subgraph Step3["Step 3: Category Routing"]
        ROUTE{"Category?"}
        REQ["RequirementEvaluator"]
        TC["TestCaseEvaluator"]
        TS["TestScriptEvaluator"]
        GEN["GenericEvaluator"]
    end

    subgraph Step4["Step 4: Metric Execution"]
        METRICS1["Get applicable metrics"]
        METRICS2["Initialize LLM Provider"]
        METRICS3["Execute DeepEval metrics<br/>(uses rendered_prompt, retrieval_context)"]
        METRICS4["Execute NLTK analyzers<br/>(uses llm_output)"]
        METRICS5["Execute Custom validators"]
    end

    subgraph Step5["Step 5: Score Aggregation"]
        AGG1["Normalize scores (0.0-1.0)"]
        AGG2["Apply category weights"]
        AGG3["Calculate weighted average"]
        AGG4["Check thresholds"]
    end

    subgraph Step6["Step 6: Report Generation"]
        RPT1["Generate HTML report"]
        RPT2["Create summary dashboard"]
        RPT3["Export JSON data"]
    end

    INIT1 --> INIT2 --> INIT3 --> INIT4
    INIT4 --> LOAD1
    LOAD1 --> LOAD2 --> LOAD3 --> LOAD4
    LOAD4 --> ROUTE

    ROUTE -->|requirement| REQ
    ROUTE -->|test_case| TC
    ROUTE -->|test_script| TS
    ROUTE -->|other| GEN

    REQ --> METRICS1
    TC --> METRICS1
    TS --> METRICS1
    GEN --> METRICS1

    METRICS1 --> METRICS2 --> METRICS3 --> METRICS4 --> METRICS5
    METRICS5 --> AGG1 --> AGG2 --> AGG3 --> AGG4
    AGG4 --> RPT1 --> RPT2 --> RPT3

    style Step1 fill:#dbeafe,stroke:#2563eb
    style Step2 fill:#dcfce7,stroke:#16a34a
    style Step3 fill:#fef3c7,stroke:#d97706
    style Step4 fill:#f3e8ff,stroke:#9333ea
    style Step5 fill:#fee2e2,stroke:#dc2626
    style Step6 fill:#e0e7ff,stroke:#6366f1
            </div>

            <h3>How Metrics Use the Fields</h3>
            
            <h4>DeepEval Metrics</h4>
            <pre><code>class FaithfulnessMetric(DeepEvalMetric):
    def _create_test_case(self, item: EvaluationItem) -> LLMTestCase:
        return LLMTestCase(
            input=item.rendered_prompt,              # Full rendered prompt
            actual_output=item.llm_output,           # LLM's response
            retrieval_context=[item.retrieval_context] if item.retrieval_context else None,
            expected_output=item.expected_output,
        )</code></pre>

            <h4>NLTK Metrics</h4>
            <pre><code>class TerminologyMetric(NLTKMetric):
    def evaluate(self, item: EvaluationItem) -> MetricResult:
        # Analyze input text
        input_text = item.rendered_prompt.lower()
        
        # Find quoted terms in the prompt
        quoted = re.findall(r'"([^"]+)"', item.rendered_prompt)
        
        # Analyze output text
        output_text = item.llm_output.lower()
        # ...</code></pre>

            <h4>Custom Metrics</h4>
            <pre><code>class TraceabilityMetric(CustomMetric):
    def _extract_expected_from_input(self, item: EvaluationItem) -> List[str]:
        # Look for requirements in the rendered prompt
        if item.rendered_prompt:
            refs = self._pattern.findall(item.rendered_prompt)
            return list(dict.fromkeys(r.upper() for r in refs))
        
        # Also check retrieval context
        if item.retrieval_context:
            refs = self._pattern.findall(item.retrieval_context)
            return list(dict.fromkeys(r.upper() for r in refs))
        
        return []</code></pre>
        </section>

        <!-- Metrics -->
        <section id="metrics">
            <h2>üìè Metric Registry System</h2>

            <h3>Metric Categories</h3>
            <div class="mermaid">
graph TB
    subgraph Registry["MetricRegistry"]
        direction TB
        REG["Central Registry"]
    end

    subgraph DeepEval["DeepEval Metrics"]
        FAITH["Faithfulness<br/>Uses: rendered_prompt,<br/>retrieval_context"]
        RELEV["Relevancy<br/>Uses: rendered_prompt,<br/>llm_output"]
        GEVAL["G-Eval<br/>Custom criteria"]
    end

    subgraph NLTK["NLTK Metrics"]
        READ["Readability<br/>Flesch-Kincaid"]
        TERM["Terminology<br/>Domain terms"]
        AMBIG["Ambiguity<br/>Vague language"]
        ACTION["Action Verbs<br/>Strong verbs"]
    end

    subgraph Custom["Custom Metrics"]
        FORMAT["Format Compliance"]
        TRACE["Traceability"]
        SYNTAX["Syntax Validation"]
        COVER["Coverage"]
    end

    REG --> DeepEval
    REG --> NLTK
    REG --> Custom

    style FAITH fill:#dbeafe,stroke:#2563eb
    style RELEV fill:#dbeafe,stroke:#2563eb
    style READ fill:#dcfce7,stroke:#16a34a
    style TERM fill:#dcfce7,stroke:#16a34a
    style FORMAT fill:#f3e8ff,stroke:#9333ea
    style TRACE fill:#f3e8ff,stroke:#9333ea
            </div>

            <h3>Default Metric Configuration</h3>
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Type</th>
                        <th>Categories</th>
                        <th>Threshold</th>
                        <th>Weight</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>faithfulness</code></td>
                        <td>DeepEval</td>
                        <td>All</td>
                        <td>0.85</td>
                        <td>0.20</td>
                    </tr>
                    <tr>
                        <td><code>relevancy</code></td>
                        <td>DeepEval</td>
                        <td>All</td>
                        <td>0.75</td>
                        <td>0.15</td>
                    </tr>
                    <tr>
                        <td><code>readability</code></td>
                        <td>NLTK</td>
                        <td>requirement, test_case</td>
                        <td>0.70</td>
                        <td>0.10</td>
                    </tr>
                    <tr>
                        <td><code>terminology</code></td>
                        <td>NLTK</td>
                        <td>requirement, test_case</td>
                        <td>0.80</td>
                        <td>0.10</td>
                    </tr>
                    <tr>
                        <td><code>ambiguity</code></td>
                        <td>NLTK</td>
                        <td>requirement, test_case</td>
                        <td>0.70</td>
                        <td>0.10</td>
                    </tr>
                    <tr>
                        <td><code>action_verbs</code></td>
                        <td>NLTK</td>
                        <td>test_case</td>
                        <td>0.70</td>
                        <td>0.10</td>
                    </tr>
                    <tr>
                        <td><code>format_compliance</code></td>
                        <td>Custom</td>
                        <td>requirement, test_case</td>
                        <td>0.85</td>
                        <td>0.15</td>
                    </tr>
                    <tr>
                        <td><code>traceability</code></td>
                        <td>Custom</td>
                        <td>test_case, test_script</td>
                        <td>0.90</td>
                        <td>0.15</td>
                    </tr>
                    <tr>
                        <td><code>syntax_correctness</code></td>
                        <td>Custom</td>
                        <td>test_script</td>
                        <td>0.95</td>
                        <td>0.20</td>
                    </tr>
                </tbody>
            </table>

            <h3>Registering Custom Metrics</h3>
            <pre><code>from prompt_eval import MetricRegistry, MetricType

# Register a custom metric
MetricRegistry.register(
    name="my_custom_metric",
    metric_class=MyCustomMetric,
    categories=["requirement", "test_case"],
    weight=0.15,
    threshold=0.80,
    is_critical=False,
    description="My custom evaluation metric",
    metric_type=MetricType.CUSTOM,
)</code></pre>
        </section>

        <!-- LLM Providers -->
        <section id="llm-providers">
            <h2>ü§ñ LLM Provider System</h2>

            <h3>Supported Providers</h3>
            <table>
                <thead>
                    <tr>
                        <th>Provider</th>
                        <th>Models</th>
                        <th>Configuration</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>OpenAI</strong></td>
                        <td>GPT-4o, GPT-4-turbo, GPT-3.5-turbo</td>
                        <td><code>OPENAI_API_KEY</code></td>
                    </tr>
                    <tr>
                        <td><strong>Azure OpenAI</strong></td>
                        <td>GPT-4, GPT-4-32k</td>
                        <td><code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code></td>
                    </tr>
                    <tr>
                        <td><strong>Anthropic</strong></td>
                        <td>Claude 3.5 Sonnet, Claude 3 Opus/Haiku</td>
                        <td><code>ANTHROPIC_API_KEY</code></td>
                    </tr>
                    <tr>
                        <td><strong>Google</strong></td>
                        <td>Gemini Pro, Gemini 1.5 Pro</td>
                        <td><code>GOOGLE_API_KEY</code></td>
                    </tr>
                    <tr>
                        <td><strong>Local (Ollama)</strong></td>
                        <td>Llama 3, Mistral, CodeLlama</td>
                        <td>Local endpoint</td>
                    </tr>
                </tbody>
            </table>

            <h3>LLM Provider Resolution Priority</h3>
            <div class="mermaid">
flowchart LR
    subgraph Priority["Resolution Priority (Highest to Lowest)"]
        direction LR
        ITEM["1Ô∏è‚É£ Item-level<br/>llm_provider/llm_model<br/>columns in Excel"]
        CAT["2Ô∏è‚É£ Category-level<br/>Configuration per<br/>category"]
        GLOBAL["3Ô∏è‚É£ Global<br/>Default<br/>configuration"]
    end

    ITEM -->|"if not set"| CAT
    CAT -->|"if not set"| GLOBAL

    style ITEM fill:#dcfce7,stroke:#16a34a
    style CAT fill:#fef3c7,stroke:#d97706
    style GLOBAL fill:#dbeafe,stroke:#2563eb
            </div>

            <h3>Configuration Example</h3>
            <pre><code>llm:
  provider: "openai"
  model: "gpt-4o"
  # Category-specific overrides
  category_overrides:
    test_script:
      provider: "anthropic"
      model: "claude-3-5-sonnet"
    requirement:
      provider: "openai"
      model: "gpt-4-turbo"</code></pre>
        </section>

        <!-- Evaluators -->
        <section id="evaluators">
            <h2>üîç Category-Specific Evaluators</h2>

            <h3>Evaluator Hierarchy</h3>
            <div class="mermaid">
classDiagram
    class BaseEvaluator {
        &lt;&lt;abstract&gt;&gt;
        +get_category() str
        +get_applicable_metrics() List
        +validate_item(item) List
        +evaluate(item, config) EvaluationResult
    }

    class RequirementEvaluator {
        +get_category() "requirement"
        +validate_item(item) List
    }

    class TestCaseEvaluator {
        +get_category() "test_case"
        +validate_item(item) List
    }

    class TestScriptEvaluator {
        +framework: str
        +get_category() "test_script"
        +validate_item(item) List
    }

    class GenericEvaluator {
        +get_category() str
        +validate_item(item) List
    }

    BaseEvaluator <|-- RequirementEvaluator
    BaseEvaluator <|-- TestCaseEvaluator
    BaseEvaluator <|-- TestScriptEvaluator
    BaseEvaluator <|-- GenericEvaluator
            </div>

            <h3>Evaluator Metrics</h3>
            <table>
                <thead>
                    <tr>
                        <th>Evaluator</th>
                        <th>Applicable Metrics</th>
                        <th>Special Validations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>RequirementEvaluator</strong></td>
                        <td>faithfulness, relevancy, readability, terminology, ambiguity, format_compliance</td>
                        <td>Warns if <code>retrieval_context</code> is missing</td>
                    </tr>
                    <tr>
                        <td><strong>TestCaseEvaluator</strong></td>
                        <td>faithfulness, relevancy, readability, action_verbs, ambiguity, traceability, format_compliance</td>
                        <td>Warns if <code>retrieval_context</code> (requirements) is missing</td>
                    </tr>
                    <tr>
                        <td><strong>TestScriptEvaluator</strong></td>
                        <td>faithfulness, relevancy, syntax_correctness, framework_compliance, traceability</td>
                        <td>Checks for code blocks, framework patterns</td>
                    </tr>
                    <tr>
                        <td><strong>GenericEvaluator</strong></td>
                        <td>faithfulness, relevancy (+ inherited base metrics)</td>
                        <td>Basic validation only</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Usage -->
        <section id="usage">
            <h2>üöÄ Usage Examples</h2>

            <h3>Command Line Interface</h3>
            <pre><code># Basic usage
prompt-eval run --input data/evaluation_data.xlsx --output reports/

# Specify LLM provider
prompt-eval run --input data.xlsx --provider openai --model gpt-4o --output reports/

# Filter by category
prompt-eval run --input data.xlsx --category requirement --output reports/

# Validate Excel structure
prompt-eval validate --input data.xlsx

# Generate sample data
prompt-eval generate-sample --output examples/sample.xlsx</code></pre>

            <h3>Python API</h3>
            <pre><code>from prompt_eval import EvaluationEngine, Config, LLMConfig

# Initialize with configuration
config = Config(
    llm=LLMConfig(
        provider="openai",
        model="gpt-4o",
        category_overrides={
            "test_script": {"provider": "anthropic", "model": "claude-3-5-sonnet"}
        }
    ),
    faithfulness_threshold=0.85,
    relevancy_threshold=0.75,
)

engine = EvaluationEngine(config)

# Run evaluation
results = engine.evaluate_from_excel("data/evaluation_data.xlsx")

# Generate report
report_path = engine.generate_report(results, "reports/evaluation_report.html")

print(f"Report generated: {report_path}")
print(f"Pass rate: {results.summary.pass_rate}%")</code></pre>

            <h3>Creating Evaluation Items Programmatically</h3>
            <pre><code>from prompt_eval.core.models import EvaluationItem

# Create an evaluation item
item = EvaluationItem(
    id="EVAL-001",
    category="requirement_analysis",
    prompt_template="""You are a requirements analyst. 
    Analyze the following user story and extract functional requirements:
    
    User Story: {{input}}
    
    Output format: REQ-XXX: [Requirement description]""",
    prompt_input="""As a customer, I want to login with my email and password 
    so that I can access my account dashboard.""",
    llm_output="""REQ-001: The system shall provide a login form with email and password fields.
    REQ-002: The system shall validate user credentials against the database.
    REQ-003: The system shall redirect authenticated users to their dashboard.""",
    retrieval_context="""Authentication Requirements Spec v2.1:
    - Users must authenticate using email and password
    - Email format: standard RFC 5322 compliant
    - Password requirements: 8+ chars, 1 uppercase, 1 number""",
)

# Access the computed rendered_prompt
print(item.rendered_prompt)
# Output: "You are a requirements analyst. Analyze the following user story..."</code></pre>

            <h3>Configuration File (config.yaml)</h3>
            <pre><code># General settings
general:
  verbose: true
  parallel: true
  max_workers: 4

# LLM Provider Configuration
llm:
  provider: "openai"
  model: "gpt-4o"
  category_overrides:
    test_script:
      provider: "anthropic"
      model: "claude-3-5-sonnet"

# Thresholds
thresholds:
  faithfulness: 0.85
  relevancy: 0.75
  readability: 0.70
  terminology: 0.80

# Metric weights
weights:
  faithfulness: 0.20
  relevancy: 0.15
  readability: 0.10
  format_compliance: 0.15

# Report settings
report:
  title: "Prompt Evaluation Report"
  include_charts: true
  include_details: true
  show_rendered_prompt: true</code></pre>
        </section>

        <!-- Summary -->
        <section id="summary">
            <h2>‚úÖ Summary</h2>

            <p>The Unified Prompt Evaluation System provides:</p>

            <div class="feature-grid">
                <div class="feature-card">
                    <h4>‚úÖ Separated Prompt Engineering</h4>
                    <p><code>prompt_template</code> + <code>prompt_input</code> for clear evaluation</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Placeholder Support</h4>
                    <p><code>{{input}}</code> substitution in templates</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Explicit Retrieval Context</h4>
                    <p><code>retrieval_context</code> for faithfulness evaluation</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Computed Rendered Prompt</h4>
                    <p><code>rendered_prompt</code> property for convenience</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Multi-LLM Provider Support</h4>
                    <p>OpenAI, Azure, Anthropic, Google, Local</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Custom Categories</h4>
                    <p>Define your own evaluation categories</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Pluggable Metrics</h4>
                    <p>DeepEval, NLTK, and custom metrics</p>
                </div>
                <div class="feature-card">
                    <h4>‚úÖ Interactive HTML Reports</h4>
                    <p>Comprehensive visualization and export</p>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p><strong>Unified Prompt Evaluation System</strong></p>
            <p>Last Updated: January 2026</p>
        </div>
    </footer>

    <script>
        // Initialize Mermaid
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            securityLevel: 'loose',
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            }
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Highlight current section in navigation
        const sections = document.querySelectorAll('section[id]');
        const navLinks = document.querySelectorAll('nav a');

        window.addEventListener('scroll', () => {
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                if (scrollY >= sectionTop - 200) {
                    current = section.getAttribute('id');
                }
            });

            navLinks.forEach(link => {
                link.style.background = '';
                link.style.color = '';
                if (link.getAttribute('href') === `#${current}`) {
                    link.style.background = '#2563eb';
                    link.style.color = 'white';
                }
            });
        });
    </script>
</body>
</html>
